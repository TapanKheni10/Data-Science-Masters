{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed1a3b0-e989-49d9-80be-75b5b625a640",
   "metadata": {},
   "source": [
    "**1) Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?**\n",
    "\n",
    "R-squared, also known as the coefficient of determination, is a statistical measure used to assess the goodness of fit of a linear regression model. It represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "Here's a small breakdown of it:\n",
    "\n",
    "1. What it represents:\n",
    "- R-squared ranges from 0 to 1 (or 0% to 100%).\n",
    "- It indicates how well the regression model fits the observed data.\n",
    "- A higher R-squared suggests that more of the variance in the dependent variable is explained by the independent variable(s).\n",
    "2. Calculation: \n",
    "R-squared is calculated using the following formula:\n",
    "- R² = 1 - (SSres / SStot)\n",
    "\n",
    "  Where:\n",
    "  - SSres is the sum of squared residuals (unexplained variance)\n",
    "  - SStot is the total sum of squares (total variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed161b5d-1315-4cc8-830e-e830457cd935",
   "metadata": {},
   "source": [
    "**2) Define adjusted R-squared and explain how it differs from the regular R-squared.**\n",
    "\n",
    "Adjusted R-squared is a modified version of R-squared that addresses some of the limitations of the regular R-squared.\n",
    "\n",
    "**Adjusted R-squared:**\n",
    "1. Definition:\n",
    "- Adjusted R-squared is a modified version of R-squared that takes into account the number of predictors in a model. It adjusts the R-squared value based on the number of independent variables relative to the sample size.\n",
    "2. Formula: The formula for adjusted R-squared is:\n",
    "- Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - k - 1)]\n",
    "  \n",
    "   Where:\n",
    "   - R² is the regular R-squared\n",
    "   - n is the number of observations\n",
    "   - k is the number of predictors (excluding the constant)\n",
    "\n",
    "3. Purpose:\n",
    "- The main purpose of adjusted R-squared is to provide a more accurate measure of model fit when comparing models with different numbers of predictors.\n",
    "\n",
    "**Differences from regular R-squared:**\n",
    "1. Penalty for complexity:\n",
    "- Regular R-squared always increases or stays the same when you add more predictors, even if they don't improve the model.\n",
    "- Adjusted R-squared penalizes the addition of unnecessary predictors. It can decrease if you add predictors that don't improve the model significantly.\n",
    "\n",
    "2. Comparison across models:\n",
    "- Regular R-squared is not ideal for comparing models with different numbers of predictors.\n",
    "- Adjusted R-squared allows for fairer comparisons between models with different numbers of predictors.\n",
    "\n",
    "3. Interpretation:\n",
    "- Regular R-squared represents the proportion of variance explained by the model.\n",
    "- Adjusted R-squared represents the proportion of variance explained by the model, adjusted for the number of predictors.\n",
    "\n",
    "4. Value range:\n",
    "- Regular R-squared is always between 0 and 1.\n",
    "- Adjusted R-squared can be negative if the model is very poor.\n",
    "\n",
    "5. Sensitivity to sample size:\n",
    "- Regular R-squared doesn't account for sample size.\n",
    "- Adjusted R-squared takes into account both the number of predictors and the sample size.\n",
    "\n",
    "6. Model selection:\n",
    "- When selecting between models, adjusted R-squared is often preferred because it helps prevent overfitting by penalizing excessive complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e410e4-7173-4efd-8d41-c98c68611238",
   "metadata": {},
   "source": [
    "**3) When is it more appropriate to use adjusted R-squared?**\n",
    "\n",
    "Whenever we want to do a fairer comparison between the models with different number of predictors because it takes into account for both the number of predictors added to the model and sample size. if you're gonna add the predictor that doesn't have huge impact on the models predictive power the value of Adjusted R-squared may decrease as well which is not the case in Regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a0027-717f-47b9-9ebd-71c369c48b45",
   "metadata": {},
   "source": [
    "**4) What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?**\n",
    "\n",
    "RMSE, MSE, and MAE are all error metrics used in regression analysis to evaluate the performance of a model. They measure the difference between predicted values and actual observed values.\n",
    "\n",
    "**MSE (Mean Squared Error):**\n",
    "- Calculation: MSE = (1/n) * Σ(yi - ŷi)²\n",
    "  Where: \n",
    "  - n is the number of observations, \n",
    "  - yi is the actual value, \n",
    "  - ŷi is the predicted value\n",
    "\n",
    "- Represents: The average of the squared differences between predicted and actual values\n",
    "- Interpretation: Lower values indicate better fit. MSE penalizes larger errors more heavily due to squaring\n",
    "\n",
    "**RMSE (Root Mean Squared Error):**\n",
    "- Calculation: RMSE = √MSE = √[(1/n) * Σ(yi - ŷi)²]\n",
    "- Represents: The square root of MSE, giving a measure of the average magnitude of the error\n",
    "- Interpretation: Lower values indicate better fit. RMSE is in the same units as the dependent variable, making it easier to interpret\n",
    "\n",
    "**MAE (Mean Absolute Error):**\n",
    "- Calculation: MAE = (1/n) * Σ|yi - ŷi|\n",
    "- Represents: The average of the absolute differences between predicted and actual values\n",
    "- Interpretation: Lower values indicate better fit. MAE is less sensitive to outliers compared to MSE and RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230fe561-8e89-4c88-aee5-365b14cb0ae1",
   "metadata": {},
   "source": [
    "**5) Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.**\n",
    "\n",
    "**MSE (Mean Squared Error):**\n",
    "- Advantages:\n",
    "  - Penalizes larger errors more heavily due to squaring, which can be desirable in many applications\n",
    "  - Mathematically tractable, making it useful for optimization algorithms\n",
    "  - Always positive, which simplifies interpretation in some contexts\n",
    "- Disadvantages:\n",
    "  - Not in the same units as the target variable, making it less intuitive to interpret\n",
    "  - More sensitive to outliers, which can skew the overall error assessment\n",
    "  \n",
    "**RMSE (Root Mean Squared Error):**\n",
    "- Advantages:\n",
    "  - In the same units as the target variable, making it more interpretable\n",
    "  - Like MSE, it penalizes larger errors more due to the squaring before taking the root\n",
    "\n",
    "- Disadvantages:\n",
    "  - Still more sensitive to outliers than MAE\n",
    "  - Can be more difficult to compute derivatives for (relevant in some optimization contexts)\n",
    "  \n",
    "**MAE (Mean Absolute Error):**\n",
    "- Advantages:\n",
    "  - Most intuitive to understand - it's the average absolute difference between predicted and actual values\n",
    "  - Less sensitive to outliers compared to MSE and RMSE\n",
    "  - In the same units as the target variable\n",
    "\n",
    "- Disadvantages:\n",
    "  - Doesn't penalize larger errors as heavily as MSE or RMSE, which might be undesirable in some applications\n",
    "  - Can be less mathematically convenient for optimization (due to the absolute value function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dcaba2-5660-4cc8-8515-e49b6d825275",
   "metadata": {},
   "source": [
    "**6) Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?**\n",
    "\n",
    "**Lasso Regularization:**\n",
    "\n",
    "Lasso stands for Least Absolute Shrinkage and Selection Operator. It's a regularization technique used in linear regression models to prevent overfitting and perform feature selection.\n",
    "\n",
    "Key aspects of Lasso:\n",
    "1. Objective function:\n",
    "- Lasso adds the L1 norm of the coefficients to the loss function.\n",
    "- Objective = Loss function + λ * Σ|βj|\n",
    "\n",
    "  Where \n",
    "  - λ is the regularization parameter, and \n",
    "  - βj are the model coefficients.\n",
    "\n",
    "2. Effect on coefficients:\n",
    "- Lasso tends to shrink some coefficients to exactly zero.\n",
    "- This results in a sparse model, effectively performing feature selection.\n",
    "\n",
    "3. Regularization parameter (λ):\n",
    "- Controls the strength of the penalty.\n",
    "- Larger λ values lead to more coefficients being pushed to zero.\n",
    "\n",
    "Differences from Ridge Regularization:\n",
    "1. Penalty term:\n",
    "- Lasso uses L1 regularization (sum of absolute values of coefficients).\n",
    "- Ridge uses L2 regularization (sum of squared values of coefficients).\n",
    "\n",
    "2. Feature selection:\n",
    "- Lasso can reduce coefficients to exactly zero, effectively selecting features.\n",
    "- Ridge typically shrinks all coefficients but rarely sets them to exactly zero.\n",
    "\n",
    "3. Solution uniqueness:\n",
    "- Lasso may not have a unique solution when predictors are highly correlated.\n",
    "- Ridge typically has a unique solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a1a34b-5990-4e2f-8d8c-30b12e3229e0",
   "metadata": {},
   "source": [
    "**7) How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.**\n",
    "\n",
    "Regularized linear models help prevent overfitting by adding a penalty term to the loss function, which discourages the model from relying too heavily on any individual feature or learning noise in the training data. This results in simpler models that generalize better to unseen data.\n",
    "\n",
    "Types of regularization:\n",
    "- L1 (Lasso): Adds the sum of absolute values of coefficients to the loss function.\n",
    "- L2 (Ridge): Adds the sum of squared values of coefficients to the loss function.\n",
    "\n",
    "Example to illustrate:\n",
    "\n",
    "Let's consider a simple scenario of predicting house prices based on square footage and number of bedrooms.\n",
    "Scenario:\n",
    "- Feature 1 (x1): Square footage (normalized)\n",
    "- Feature 2 (x2): Number of bedrooms\n",
    "- Target (y): House price (in $100,000s)\n",
    "\n",
    "Suppose we have the following small dataset: \n",
    "\n",
    "x1 = [1.0, 1.2, 1.1, 0.9, 1.3], x2 = [3, 3, 4, 2, 4], y = [5, 6, 5.5, 4.5, 7]\n",
    "\n",
    "Unregularized linear regression might yield: **y = 2.5x1 + 1.2x2 + 0.1**\n",
    "\n",
    "This model fits the training data well but might not generalize to new data.\n",
    "\n",
    "Now, let's apply regularization (let's say Ridge regression with α = 1):\n",
    "\n",
    "Regularized model: **y = 1.8x1 + 0.9x2 + 0.3**\n",
    "\n",
    "Observations:\n",
    "- Coefficient magnitudes are smaller in the regularized model.\n",
    "- The regularized model is less sensitive to small changes in input features.\n",
    "- The intercept (0.3) is larger, indicating less reliance on the features.\n",
    "\n",
    "To illustrate overfitting prevention, consider a new data point:\n",
    "\n",
    "x1 = [1.4], x2 = [5]\n",
    "\n",
    "Unregularized prediction: 2.5(1.4) + 1.2(5) + 0.1 = 9.6 ($960,000)\n",
    "\n",
    "Regularized prediction: 1.8(1.4) + 0.9(5) + 0.3 = 7.2 ($720,000)\n",
    "\n",
    "The regularized model's prediction is likely more realistic and generalizable, especially if the training data was limited or noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fcf263-a15d-4762-8350-0f91820d189c",
   "metadata": {},
   "source": [
    "**8) Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.**\n",
    "\n",
    "While regularized linear models like Lasso and Ridge regression are powerful tools in machine learning, they do have limitations. Understanding these can help in choosing the most appropriate model for a given regression analysis task. Let's discuss the limitations and reasons why regularized linear models may not always be the best choice:\n",
    "\n",
    "1. Linearity Assumption:\n",
    "- Limitation: Regularized linear models assume a linear relationship between features and the target variable.\n",
    "- Problem: In many real-world scenarios, relationships can be non-linear.\n",
    "- Consequence: May miss important non-linear patterns in the data.\n",
    "\n",
    "2. Outlier Sensitivity:\n",
    "- Limitation: Although regularization helps, these models can still be sensitive to outliers.\n",
    "- Problem: Extreme values can disproportionately influence the model.\n",
    "- Consequence: May lead to skewed predictions if outliers are not properly handled.\n",
    "\n",
    "3. Feature Scale Dependency:\n",
    "- Limitation: The effect of regularization depends on the scale of features.\n",
    "- Problem: Features with larger scales may be penalized more heavily.\n",
    "- Consequence: Requires careful feature scaling to ensure fair regularization.\n",
    "\n",
    "4. Hyperparameter Tuning:\n",
    "- Limitation: Requires careful tuning of the regularization parameter.\n",
    "- Problem: Optimal regularization strength can vary significantly between datasets.\n",
    "- Consequence: Adds complexity to the modeling process and may require extensive cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23466d2-9c2c-4bd1-93db-abb4a326fb24",
   "metadata": {},
   "source": [
    "**9) You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?**\n",
    "\n",
    "Metric Comparison:\n",
    "- First, it's crucial to note that we're comparing different metrics - RMSE (Root Mean Square Error) for Model A and MAE (Mean Absolute Error) for Model B. These metrics cannot be directly compared as they have different properties and interpretations. \n",
    "\n",
    "Properties of RMSE and MAE:\n",
    "- RMSE: More sensitive to large errors due to squaring\n",
    "- MAE: Treats all errors linearly, less sensitive to outliers\n",
    "\n",
    "Scale of Errors:\n",
    "- Without knowing the scale of the target variable, it's hard to interpret whether an RMSE of 10 or an MAE of 8 is good or bad.\n",
    "\n",
    "Lack of Common Metric:\n",
    "- To make a fair comparison, we would need the same metric for both models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3940f043-26f5-4b79-8fc5-d6f0f2222036",
   "metadata": {},
   "source": [
    "**10) You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?**\n",
    "\n",
    "To compare these two models effectively, we need to consider several factors beyond just the regularization type and parameter value. Let's break down the scenario and discuss the considerations:\n",
    "\n",
    "Regularization Types:\n",
    "- Model A: Ridge (L2) regularization\n",
    "- Model B: Lasso (L1) regularization\n",
    "\n",
    "Regularization Parameters:\n",
    "- Model A: 0.1\n",
    "- Model B: 0.5\n",
    "\n",
    "However, it's important to note that we can't directly compare these models based solely on this information. Here's why, and what we need to consider:\n",
    "\n",
    "Performance Metrics:\n",
    "- We don't have any performance metrics (e.g., RMSE, MAE, R-squared) for either model. These are crucial for comparing model performance.\n",
    "\n",
    "Dataset Characteristics:\n",
    "- We don't know about the dataset - its size, number of features, presence of multicollinearity, etc.\n",
    "\n",
    "Different Scales:\n",
    "- The regularization parameters (0.1 and 0.5) are not directly comparable between Ridge and Lasso, as they can have different effects depending on the dataset and model.\n",
    "\n",
    "Objective:\n",
    "- The choice between Ridge and Lasso often depends on the specific goals of the analysis.\n",
    "\n",
    "Given these limitations, we can't definitively choose a \"better performer.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

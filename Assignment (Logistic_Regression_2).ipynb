{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6401d1e-1cab-4233-9d00-eafa6d512b89",
   "metadata": {},
   "source": [
    "**1) What is the purpose of grid search cv in machine learning, and how does it work?**\n",
    "\n",
    "Grid search CV (Cross-Validation) is a technique used in machine learning for hyperparameter tuning.\n",
    "\n",
    "**Purpose:**\n",
    "- Find optimal hyperparameters for a model\n",
    "- Improve model performance\n",
    "- Reduce overfitting\n",
    "- Automate the process of model tuning\n",
    "\n",
    "**How it works:**\n",
    "- Define a grid of hyperparameter values\n",
    "- Create models with all possible combinations of these values\n",
    "- Evaluate each model using cross-validation\n",
    "- Select the best-performing combination of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa386f5c-2f4d-4504-88e6-1fecfb147b6f",
   "metadata": {},
   "source": [
    "**2) Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?**\n",
    "\n",
    "Grid search CV and randomized search CV are both hyperparameter tuning techniques, but they differ in their approach:\n",
    "\n",
    "**Grid Search CV:**\n",
    "- Exhaustively searches through a specified set of hyperparameters\n",
    "- Tests all possible combinations\n",
    "\n",
    "**Randomized Search CV:**\n",
    "- Randomly samples from the hyperparameter space\n",
    "- Tests a specified number of combinations\n",
    "\n",
    "Key differences:\n",
    "\n",
    "**Search strategy:**\n",
    "- Grid: Systematic and complete\n",
    "- Random: Stochastic and partial\n",
    "\n",
    "**Efficiency:**\n",
    "- Grid: Can be computationally expensive\n",
    "- Random: Often more efficient, especially for large parameter spaces\n",
    "\n",
    "**Coverage:**\n",
    "- Grid: May miss optimal values between grid points\n",
    "- Random: Can find unexpected combinations, better for continuous parameters\n",
    "\n",
    "When to choose:\n",
    "\n",
    "**Choose Grid Search when:**\n",
    "- Parameter space is small\n",
    "- You have specific values to test\n",
    "- Computational resources are not a constraint\n",
    "\n",
    "**Choose Randomized Search when:**\n",
    "- Parameter space is large or continuous\n",
    "- You're unsure about the range of good parameters\n",
    "- Computational resources are limited\n",
    "- You want to explore the parameter space more broadly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4ae575-7974-47dd-a3e7-9436f102bfe3",
   "metadata": {},
   "source": [
    "**3) What is data leakage, and why is it a problem in machine learning? Provide an example.**\n",
    "\n",
    "Data leakage occurs when information from outside the training dataset is used to create the model, leading to overly optimistic performance estimates.\n",
    "\n",
    "Why it's a problem:\n",
    "- Overestimates model performance\n",
    "- Creates unrealistic expectations\n",
    "- Model fails to generalize to new, unseen data\n",
    "- Can lead to incorrect business decisions\n",
    "\n",
    "Example:\n",
    "\n",
    "In a credit default prediction model:\n",
    "- Dataset includes a feature \"account_closed\" (Yes/No)\n",
    "- This feature is highly correlated with default status\n",
    "- However, account closure often happens after a default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dee714-d505-4369-851b-eded00d90183",
   "metadata": {},
   "source": [
    "**4) How can you prevent data leakage when building a machine learning model?**\n",
    "\n",
    "Let's consider an above mentioned example.\n",
    "\n",
    "The problem:\n",
    "\n",
    "Using \"account_closed\" in the model leaks information from the future, making predictions unrealistically accurate but useless for real-world applications.\n",
    "\n",
    "Prevention:\n",
    "- Careful feature engineering\n",
    "- Proper train-test splitting\n",
    "- Time-based validation for time-series data\n",
    "- Cross-validation\n",
    "- Domain expertise to identify potential leakage sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bc056a-166b-4865-8d65-1885bad3f549",
   "metadata": {},
   "source": [
    "**5) What is a confusion matrix, and what does it tell you about the performance of a classification model?**\n",
    "\n",
    "A confusion matrix is a table that summarizes the performance of a classification model.\n",
    "\n",
    "Here's a concise explanation:\n",
    "\n",
    "A tabular summary of predicted vs. actual class outcomes.\n",
    "Structure (for binary classification):\n",
    "\n",
    "| | Predicted Positive | Predicted Negative |\n",
    "|-------------------|--------------------|--------------------|\n",
    "| Actual Positive   | True Positive (TP)  | False Negative (FN)|\n",
    "| Actual Negative   | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "What it tells you:\n",
    "\n",
    "**Correct predictions:**\n",
    "- TP: Correctly predicted positives\n",
    "- TN: Correctly predicted negatives\n",
    "\n",
    "**Errors:**\n",
    "- FP: Incorrectly predicted positives\n",
    "- FN: Incorrectly predicted negatives\n",
    "\n",
    "We can derived certain important metrices like Accuracy, Precision, Recall and F1-score etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a3020-1f71-486b-ae09-5999215f52b8",
   "metadata": {},
   "source": [
    "**6) Explain the difference between precision and recall in the context of a confusion matrix.**\n",
    "\n",
    "**Precision:**\n",
    "- The proportion of correct positive predictions among all positive predictions\n",
    "- Formula: **TP / (TP + FP)**\n",
    "- Focus: Measures the accuracy of positive predictions\n",
    "- Interpretation: \"When the model predicts positive, how often is it correct?\"\n",
    "\n",
    "**Recall (also known as Sensitivity):**\n",
    "- The proportion of actual positives that were correctly identified\n",
    "- Formula: **TP / (TP + FN)**\n",
    "- Focus: Measures the model's ability to find all positive instances\n",
    "- Interpretation: \"Of all the actual positive cases, how many did the model correctly identify?\"\n",
    "\n",
    "**Key differences:**\n",
    "- Precision prioritizes minimizing false positives\n",
    "- Recall prioritizes minimizing false negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff30dc0c-346d-476b-879b-40418a46ccbf",
   "metadata": {},
   "source": [
    "**7) How can you interpret a confusion matrix to determine which types of errors your model is making?**\n",
    "\n",
    "Interpreting a confusion matrix helps identify the types of errors your model is making.\n",
    "\n",
    "Here's how to analyze it:\n",
    "\n",
    "**True Positives (TP):** \n",
    "- Correct positive predictions\n",
    "- Upper left cell\n",
    "\n",
    "**True Negatives (TN):** \n",
    "- Correct negative predictions\n",
    "- Lower right cell\n",
    "\n",
    "**False Positives (FP) - Type I error:**\n",
    "- Lower left cell\n",
    "- Model incorrectly predicts positive\n",
    "- Indicates over-prediction of positive class\n",
    "\n",
    "**False Negatives (FN) - Type II error:**\n",
    "- Upper right cell\n",
    "- Model incorrectly predicts negative\n",
    "- Indicates under-prediction of positive class\n",
    "\n",
    "Interpretation:\n",
    "- High FP: Model is too sensitive, over-predicting positives\n",
    "- High FN: Model is too specific, under-predicting positives\n",
    "- Compare FP vs FN to see which error type is more common\n",
    "- Look at the ratio of errors to correct predictions in each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dc0d59-6b34-41f1-884e-d7028582f931",
   "metadata": {},
   "source": [
    "**8) What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?**\n",
    "\n",
    "Derived metrics:\n",
    "- Accuracy: (TP + TN) / Total\n",
    "- Precision: TP / (TP + FP)\n",
    "- Recall: TP / (TP + FN)\n",
    "- F1-score: Harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8004c0-cc3e-4a07-8a6e-1fa198afecd6",
   "metadata": {},
   "source": [
    "**9) What is the relationship between the accuracy of a model and the values in its confusion matrix?**\n",
    "\n",
    "The proportion of correct predictions (both true positives and true negatives) among the total number of cases examined.\n",
    "\n",
    "Accuracy formula:\n",
    "- Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "  Where:\n",
    "  - TP = True Positives\n",
    "  - TN = True Negatives\n",
    "  - FP = False Positives\n",
    "  - FN = False Negatives\n",
    "\n",
    "Relationship:\n",
    "- Higher TP and TN values increase accuracy\n",
    "- Higher FP and FN values decrease accuracy\n",
    "- Accuracy is the sum of the diagonal elements (TP + TN) divided by the sum of all elements in the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc64865b-f06e-45c7-9ae8-f655bbd5f494",
   "metadata": {},
   "source": [
    "**10) How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?**\n",
    "\n",
    "A confusion matrix can reveal several biases and limitations in a machine learning model. Here's how to use it for this purpose:\n",
    "\n",
    "**Class imbalance bias:**\n",
    "- Compare row totals (actual class distributions)\n",
    "- If one class is much larger, the model may be biased towards it\n",
    "\n",
    "**Prediction bias:**\n",
    "- Compare column totals to row totals\n",
    "- If significantly different, model may over/under-predict certain classes\n",
    "\n",
    "**Specific class performance:**\n",
    "- Examine TP, TN, FP, FN for each class\n",
    "- Poor performance in one class suggests limitations for that category\n",
    "\n",
    "**Error types:**\n",
    "- Compare FP and FN\n",
    "- Consistent misclassification between specific classes indicates model limitations\n",
    "\n",
    "**Overall accuracy vs. class-specific accuracy:**\n",
    "- High overall accuracy with poor performance in minority classes suggests bias\n",
    "\n",
    "**Precision and recall trade-offs:**\n",
    "- Imbalanced precision and recall may indicate bias towards certain outcomes\n",
    "\n",
    "**Cost-sensitive errors:**\n",
    "- If certain misclassifications are more costly, assess their frequency\n",
    "\n",
    "**Threshold bias:**\n",
    "- Adjust classification threshold and observe changes in TP, FP, TN, FN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b180d59-b9d1-4fd4-81e6-359f727bcffb",
   "metadata": {},
   "source": [
    "**1) Explain the difference between linear regression and logistic regression models. Provide an example of\n",
    "a scenario where logistic regression would be more appropriate.**\n",
    "\n",
    "Linear regression ans Logistic regression differs primarily in their use-cases and outputs:\n",
    "\n",
    "**Linear regression:**\n",
    "- Predicts continuous numerical values meaning to say it is used for regression task.\n",
    "- Output is a linear combination of input features\n",
    "\n",
    "**Logistic regression:**\n",
    "- Predicts probability of binary outcomes (0 or 1) meaning to say it is used for classification task.\n",
    "- Output is transformed using logistic or sigmoid function\n",
    "\n",
    "Example scenario for logistic regression:\n",
    "Predicting whether a customer will purchase a product (yes/no) based on factors like age, income, and previous buying behavior. Here, the outcome is binary, making logistic regression more appropriate than linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adac41b-8938-4186-a207-b4d39e7d6424",
   "metadata": {},
   "source": [
    "**2) What is the cost function used in logistic regression, and how is it optimized?**\n",
    "\n",
    "The cost function used in logistic regression is the log loss (also known as cross-entropy loss). It's optimized using gradient descent.\n",
    "\n",
    "Here's is a brief explanation:\n",
    "1. Cost function (Log loss): **J(θ) = -1/m * Σ[y*log(h(x)) + (1-y)*log(1-h(x))]**\n",
    "   \n",
    "   Where:\n",
    "    - m is the number of training examples\n",
    "    - y is the actual label (0 or 1)\n",
    "    - h(x) is the predicted probability\n",
    "    \n",
    "2. Optimization:\n",
    "- Gradient descent is used to minimize this cost function\n",
    "- It iteratively updates the model parameters (weights)\n",
    "- The goal is to find parameters that result in the lowest cost\n",
    "\n",
    "The process involves calculating the gradient of the cost function and updating the parameters in the opposite direction of the gradient to minimize the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2741d4-3204-4a99-ac11-b4cc348322c0",
   "metadata": {},
   "source": [
    "**3) Explain the concept of regularization in logistic regression and how it helps prevent overfitting.**\n",
    "\n",
    "Regularization in logistic regression helps prevent overfitting by adding a penalty term to the cost function.\n",
    "\n",
    "**Purpose:**\n",
    "- Discourages complex models\n",
    "- Reduces model's sensitivity to individual data points\n",
    "\n",
    "\n",
    "**Types:**\n",
    "- L1 (Lasso): Adds absolute value of coefficients\n",
    "- L2 (Ridge): Adds squared value of coefficients\n",
    "\n",
    "\n",
    "**How it works:**\n",
    "- Modifies the cost function: J(θ) = LogLoss + λ * (penalty term)\n",
    "- λ (lambda) controls the strength of regularization\n",
    "- Larger λ = more regularization, simpler model\n",
    "\n",
    "\n",
    "**Effect:**\n",
    "- Shrinks coefficients towards zero\n",
    "- Reduces model complexity\n",
    "- Improves generalization to new data\n",
    "\n",
    "By penalizing large coefficients, regularization helps the model focus on the most important features and reduces its tendency to fit noise in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa9b1b7-3832-42d5-b2a3-bfa7fa0db948",
   "metadata": {},
   "source": [
    "**4) What is the ROC curve, and how is it used to evaluate the performance of the logistic regression\n",
    "model?**\n",
    "\n",
    "The ROC (receiver operating characteristic) curve is a graphical tool used to evaluate the performance of binary classification models like logistic regression.\n",
    "\n",
    "It plots **True Positive Rate (TPR) vs False Positive Rate (FPR).**\n",
    "- TPR = Sensitivity = TP / (TP + FN)\n",
    "- FPR = 1 - Specificity = FP / (FP + TN)\n",
    "\n",
    "It basically varies the classification threshold from 0 to 1 and plot the values of TPR and FPR for each threshold.\n",
    "\n",
    "Usually, the curve closer to top-left corner indicates better performance.\n",
    "\n",
    "Here's are some of the applications:\n",
    "- Compare different models\n",
    "- Choose optimal threshold for classification\n",
    "- Assess model's discriminative ability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865bf97a-6dca-4907-8c88-0a1ac76fcb20",
   "metadata": {},
   "source": [
    "**5) What are some common techniques for feature selection in logistic regression? How do these\n",
    "techniques help improve the model's performance?**\n",
    "\n",
    "Common feature selection techniques for logistic regression include:\n",
    "\n",
    "**Univariate selection:**\n",
    "- Uses statistical tests (chi-squared, ANOVA) to select best features\n",
    "- Helps identify individually significant features\n",
    "\n",
    "**Recursive Feature Elimination (RFE):**\n",
    "- Iteratively removes least important features\n",
    "- Helps find the optimal subset of features\n",
    "\n",
    "**L1 Regularization (Lasso):**\n",
    "- Shrinks less important feature coefficients to zero\n",
    "- Performs feature selection implicitly\n",
    "\n",
    "**Feature importance from tree-based models:**\n",
    "- Use random forests or decision trees to rank features\n",
    "- Captures non-linear relationships\n",
    "\n",
    "**Correlation analysis:**\n",
    "- Removes highly correlated features\n",
    "- Reduces multicollinearity\n",
    "\n",
    "These techniques improve model performance by:\n",
    "- Reducing overfitting\n",
    "- Enhancing interpretability\n",
    "- Decreasing computational complexity\n",
    "- Improving generalization\n",
    "- Focusing on most relevant predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a4dc9-ee91-47e2-91dc-21375d7d7bd9",
   "metadata": {},
   "source": [
    "**6) How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing\n",
    "with class imbalance?**\n",
    "\n",
    "Handling imbalanced datasets in logistic regression is crucial for model performance. Here are some strategies:\n",
    "\n",
    "**Resampling techniques:**\n",
    "- Oversampling minority class (e.g., SMOTE)\n",
    "- Undersampling majority class\n",
    "- Combination of both\n",
    "\n",
    "**Class weighting:**\n",
    "- Assign higher weights to minority class in the cost function\n",
    "\n",
    "**Adjusting decision threshold:**\n",
    "- Move threshold to favor minority class predictions\n",
    "\n",
    "**Ensemble methods:**\n",
    "- Bagging or boosting with focus on minority class\n",
    "\n",
    "**Collect more data:**\n",
    "- Particularly for minority class, if possible\n",
    "\n",
    "**Use appropriate evaluation metrics:**\n",
    "- F1-score, precision-recall curve, or AUC-ROC\n",
    "\n",
    "**Synthetic data generation:**\n",
    "- Create artificial examples of minority class\n",
    "\n",
    "**Anomaly detection:**\n",
    "- Treat minority class as anomalies if imbalance is extreme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec48de-5d0e-4c94-9a80-bb40f9f49e60",
   "metadata": {},
   "source": [
    "**7) Can you discuss some common issues and challenges that may arise when implementing logistic\n",
    "regression, and how they can be addressed? For example, what can be done if there is multicollinearity\n",
    "among the independent variables?**\n",
    "\n",
    "Here are some common issues in logistic regression and their solutions:\n",
    "\n",
    "**Multicollinearity:**\n",
    "- **Issue:** High correlation between independent variables\n",
    "\n",
    "Solutions:\n",
    "\n",
    "a) Remove highly correlated features\n",
    "\n",
    "b) Use regularization (L1 or L2)\n",
    "\n",
    "c) Principal Component Analysis (PCA)\n",
    "\n",
    "d) Combine correlated features\n",
    "\n",
    "**Outliers:**\n",
    "- **Issue:** Extreme values skewing the model\n",
    "\n",
    "Solutions:\n",
    "\n",
    "a) Remove or winsorize outliers\n",
    "\n",
    "b) Use robust logistic regression\n",
    "\n",
    "c) Transform variables (e.g., log transformation)\n",
    "\n",
    "**Non-linearity:**\n",
    "- **Issue:** Assuming linear relationships when they're not\n",
    "\n",
    "Solutions:\n",
    "\n",
    "a) Add polynomial terms or interaction terms\n",
    "\n",
    "b) Use non-linear transformations of features\n",
    "\n",
    "c) Consider non-linear models (e.g., decision trees)\n",
    "\n",
    "**Small sample size:**\n",
    "- **Issue:** Insufficient data for reliable estimates\n",
    "\n",
    "Solutions:\n",
    "\n",
    "a) Collect more data\n",
    "\n",
    "b) Use regularization\n",
    "\n",
    "c) Employ cross-validation\n",
    "\n",
    "**Complete separation:**\n",
    "- **Issue:** Perfect prediction of outcomes\n",
    "\n",
    "Solutions:\n",
    "\n",
    "a) Use penalized likelihood methods\n",
    "\n",
    "b) Combine rare categories\n",
    "\n",
    "c) Consider exact logistic regression\n",
    "\n",
    "**Overfitting:**\n",
    "- **Issue:** Model performs well on training data but poorly on new data\n",
    "\n",
    "Solutions:\n",
    "\n",
    "a) Use regularization\n",
    "\n",
    "b) Reduce model complexity\n",
    "\n",
    "c) Increase training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
